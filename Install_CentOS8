Tutorial discribed "how to steps" for building three nodes kubernetes cluster on CenOS 8 (On Prem or Cloud) server

Prerequisites

1. Three CentOS 8 server , One for Master and two for Workers
2. Recomended that nodes are at least 2 CPU and 4 GB RAM
3. Internet connectivity on each node
4. Inter nommunication between nodes 

Following are three nodes with IP adress

master-node 192.168.10.1
node-1      192.168.10.2
node-2      192.168.10.3

Configure Node name and IP adress (/etc/hosts)

Change hostname for all three nodes

# hostnamectl set-hostname master-node
# hostnamectl set-hostname node-1
# hostnamectl set-hostname node-2

Update /etc/hosts file for all three nodes 

#cat <<EOF>> /etc/hosts
 192.168.10.1 master-node
 192.168.10.2 node-1 worker-node-1
 192.168.10.3 node-2 worker-node-2
 EOF

Make sure each node able to ping other nodes and self using hostname


Disable Selinux:

# setenforce 0
setenforce 0 will disable Selinux untill next reboot. To disable Selinux permanently follow steps below.

# sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
# reboot


Kubernetes cluster makes verius ports for communication; access to these ports not be limited by firewall ; To ensure ports are not blocked; execute following commands 

# firewall-cmd --permanent --add-port=6443/tcp
# firewall-cmd --permanent --add-port=2379-2380/tcp
# firewall-cmd --permanent --add-port=10250/tcp
# firewall-cmd --permanent --add-port=10251/tcp
# firewall-cmd --permanent --add-port=10252/tcp
# firewall-cmd --permanent --add-port=10255/tcp
# firewall-cmd --reload
# modprobe br_netfilter
# echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables


Install Docker
=============

Add docker repo 
dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo

Insall containerd.io
# dnf install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm

Now install, enable and start docker
# dnf install docker-ce
# systemctl enable docker
# systemctl start docker

Change docker cgroups driver from cgroupfs to systemd which require because kubelet use systemd as cgroup drivers; this mismatch will fail kubelet to start

# cat <<EOF> /etc/docker/daemon.json
  {
    "exec-opts": ["native.cgroupdriver=systemd"]
  }
  EOF
#systemctl restart docker

Check Docker Info and ensure cgroup drivers updated as systemd

[root@master-node cloud_user]# docker info
Client:
 Context:    default
 Debug Mode: false
 ---
 ---
Server:
 Containers: 35
  Running: 17
  Paused: 0
  Stopped: 18
 Images: 9
 Server Version: 20.10.10
 Storage Driver: overlay2
  Backing Filesystem: xfs
  Supports d_type: true
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
------
------
[root@master-node cloud_user]#



Install Kubernetes(kubeadm) => Kubeadm helps you to bootstrap kubernetes cluster

Add kubernetes repos

# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


Afrer repo add, Install kubeadm 

# dnf install kubeadm -y 


When Installation complete; start kubelet 

systemctl enable kubelet 
systemctl start kubelet 

Note: 
You will get error that kubelet not started and missing file /var/lib/kubelet/config.yaml error; because this filw will generate during "kubeadm init"; 
No worries continue next steps 


Create Control Plane

Now time for initiating bootstraping. initializing kubernetes master using "kuneadm init"

Disable swap? kubernetes require disable of swap memories on all nodes as it can lead the stability and performance issue within kubernetes. 
Execute following commands in all three nodes to disable swap memory 

# swapoff -a
# sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab


Initialized Kubernetes at Master 

# kubeadm init
or 
# kubeadm init --pod-network-cidr <pod_nw_cidr>

When initialization complete, you will get below like string; copy and save it somewhere, require for worker node to join Master.


kubeadm join 192.168.10.1:6443 --token 9j9sqi.pw1cypm6a3vzktzd --discovery-token-ca-cert-hash sha256:3c3391c0d6c23f8874d10fcb8c901b81c440667c0554c1345f46a33a426ca619



After Initialization complete; follow steps below on Master node to enable user to start using cluster

# mkdir -p $HOME/.kube
# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# chown $(id -u):$(id -g) $HOME/.kube/config

Check the cluster status


[root@master-node cloud_user]# kubectl get nodes
NAME          STATUS   ROLES                  AGE    VERSION
master-node   NotReady    control-plane,master   158m   v1.22.3
[root@master-node cloud_user]#

Master node is in NotReady state because Pod network plugin not yet installed.
There are different plugin available, commonly used plugins are Calico, Weavenet. We will use Weavenet for this demo

# export kubever=$(kubectl version | base64 | tr -d '\n')
# kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"



Adding Worker nodes 
From node-1 and node-2; only need to execute following "kuneadm join" command as all prerequisites and require steps are completed during above steps.

Login to the both worker nodes and execute following command; once complete check kubectl get nodes to verify nodes added .

# kubeadm join 192.168.10.1:6443 --token 9j9sqi.pw1cypm6a3vzktzd --discovery-token-ca-cert-hash sha256:3c3391c0d6c23f8874d10fcb8c901b81c440667c0554c1345f46a33a426ca619


# [root@master-node cloud_user]# kubectl get nodes
  NAME          STATUS   ROLES                  AGE    VERSION
  master-node   Ready    control-plane,master   167m   v1.22.3
  node-1        NotReady    <none>                 160m   v1.22.3
  node-2        NotReady    <none>                 107m   v1.22.3
  [root@master-node cloud_user]#


If you see node-1 and node-2 are in NotReady state; then because you need install network plugin once again to replicate accross all nodes.


After re-executing network plugin installation steps, Cluster will be completely up and running with all nodes are in Ready state.

# [root@master-node cloud_user]# kubectl get nodes
  NAME          STATUS   ROLES                  AGE    VERSION
  master-node   Ready    control-plane,master   167m   v1.22.3
  node-1        Ready    <none>                 160m   v1.22.3
  node-2        Ready    <none>                 107m   v1.22.3
  [root@master-node cloud_user]#





